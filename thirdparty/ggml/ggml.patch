diff --git a/examples/llava/clip.cpp b/examples/llava/clip.cpp
index 9129052a..482a84e5 100644
--- a/examples/llava/clip.cpp
+++ b/examples/llava/clip.cpp
@@ -768,6 +768,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
 
     // kv
     const int n_kv = gguf_get_n_kv(ctx);
+    if (verbosity >= 1) {
     printf("%s: loaded meta data with %d key-value pairs and %d tensors from %s\n",
         __func__, n_kv, n_tensors, fname);
     {
@@ -807,6 +808,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
             printf("%s: - type %4s: %4d tensors\n", __func__, ggml_type_name(kv.first), kv.second);
         }
     }
+    }
 
     // data
     size_t buffer_size = 0;
@@ -848,18 +850,18 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
 
 #ifdef GGML_USE_CUBLAS
     new_clip->backend = ggml_backend_cuda_init(0);
-    printf("%s: CLIP using CUDA backend\n", __func__);
+    if (verbosity >= 1) printf("%s: CLIP using CUDA backend\n", __func__);
 #endif
 
 #ifdef GGML_USE_METAL
     new_clip->backend = ggml_backend_metal_init();
-    printf("%s: CLIP using Metal backend\n", __func__);
+    if (verbosity >= 1) printf("%s: CLIP using Metal backend\n", __func__);
 #endif
 
 
     if (!new_clip->backend) {
         new_clip->backend = ggml_backend_cpu_init();
-        printf("%s: CLIP using CPU backend\n", __func__);
+        if (verbosity >= 1) printf("%s: CLIP using CPU backend\n", __func__);
     }
 
     // model size and capabilities
@@ -891,7 +893,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         }
     }
 
-    printf("%s: params backend buffer size = % 6.2f MB (%i tensors)\n", __func__, buffer_size / (1024.0 * 1024.0), n_tensors);
+    if (verbosity >= 1) printf("%s: params backend buffer size = % 6.2f MB (%i tensors)\n", __func__, buffer_size / (1024.0 * 1024.0), n_tensors);
 
     // load tensors
     {
@@ -1086,7 +1088,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         new_clip->compute_buffer = ggml_backend_alloc_buffer(new_clip->backend, compute_memory_buffer_size);
         new_clip->compute_alloc = ggml_allocr_new_from_buffer(new_clip->compute_buffer);
 
-        printf("%s: compute allocated memory: %.2f MB\n", __func__, compute_memory_buffer_size /1024.0/1024.0);
+        if (verbosity >= 1) printf("%s: compute allocated memory: %.2f MB\n", __func__, compute_memory_buffer_size /1024.0/1024.0);
     }
 
     return new_clip;
diff --git a/examples/llava/llava.cpp b/examples/llava/llava.cpp
index d42e7582..30833df0 100644
--- a/examples/llava/llava.cpp
+++ b/examples/llava/llava.cpp
@@ -31,7 +31,7 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
     const int64_t t_img_enc_end_us = ggml_time_us();
     float t_img_enc_ms = (t_img_enc_end_us - t_img_enc_start_us) / 1000.0;
 
-    printf("\n%s: image encoded in %8.2f ms by CLIP (%8.2f ms per image patch)\n", __func__, t_img_enc_ms, t_img_enc_ms / *n_img_pos);
+    // printf("\n%s: image encoded in %8.2f ms by CLIP (%8.2f ms per image patch)\n", __func__, t_img_enc_ms, t_img_enc_ms / *n_img_pos);
 
     return true;
 }
diff --git a/llama.cpp b/llama.cpp
index 02b0a485..2088b5a9 100644
--- a/llama.cpp
+++ b/llama.cpp
@@ -11516,7 +11516,9 @@ static void llama_log_internal(ggml_log_level level, const char * format, ...) {
 
 static void llama_log_callback_default(ggml_log_level level, const char * text, void * user_data) {
     (void) level;
-    (void) user_data;
-    fputs(text, stderr);
-    fflush(stderr);
+    bool * enable_log = static_cast<bool *>(user_data);
+    if (enable_log && *enable_log) {
+        fputs(text, stderr);
+        fflush(stderr);
+    }
 }
